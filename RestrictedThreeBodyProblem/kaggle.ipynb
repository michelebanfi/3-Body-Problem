{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8833374,"sourceType":"datasetVersion","datasetId":5315370},{"sourceId":8848554,"sourceType":"datasetVersion","datasetId":5326044}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir Media","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:36:04.265144Z","iopub.execute_input":"2024-07-03T10:36:04.265508Z","iopub.status.idle":"2024-07-03T10:36:05.227477Z","shell.execute_reply.started":"2024-07-03T10:36:04.265476Z","shell.execute_reply":"2024-07-03T10:36:05.226166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom decimal import Decimal","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:36:05.229827Z","iopub.execute_input":"2024-07-03T10:36:05.230160Z","iopub.status.idle":"2024-07-03T10:36:07.920637Z","shell.execute_reply.started":"2024-07-03T10:36:05.230109Z","shell.execute_reply":"2024-07-03T10:36:07.919724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classes","metadata":{}},{"cell_type":"code","source":"class ESNReservoir(nn.Module):\n    def __init__(self, input_size, reservoir_size, output_size, pred_len, spectral_radius=0.9, sparsity=0.1):\n        super(ESNReservoir, self).__init__()\n        self.input_size = input_size\n        self.reservoir_size = reservoir_size\n        self.output_size = output_size\n        self.pred_len = pred_len\n\n        # Input weights\n        self.Win = nn.Parameter(torch.randn(reservoir_size, input_size), requires_grad=False)\n\n        # Reservoir weights\n        self.W = nn.Parameter(torch.randn(reservoir_size, reservoir_size), requires_grad=False)\n\n        # Output weights\n        self.Wout = nn.Linear(reservoir_size, output_size)\n\n        # Adjust spectral radius\n        self.W.data *= spectral_radius / torch.max(torch.abs(torch.linalg.eigvals(self.W.data)))\n\n        # Apply sparsity\n        mask = (torch.rand(reservoir_size, reservoir_size) < sparsity).float()\n        self.W.data *= mask\n\n\n    def forward(self, x):\n        device = x.device\n        h = torch.zeros(1, self.reservoir_size).to(device)\n        input_len = x.size(1)\n\n        for t in range(input_len + self.pred_len-1):\n            # take single point\n            input = x[0, t, :]\n            input = input.unsqueeze(0)\n            # get hidden state from the point extracted and the previous hidden state\n            h = F.leaky_relu(self.Win @ input.T + self.W @ h.T).T\n            # if the time reached the input lenght we start concatenating outputs in order to pick them in the next round\n            if t >= input_len-1:\n                # calculate the output\n                output = self.Wout(h)\n                output = output.unsqueeze(1)\n                x = torch.cat((x, output), dim=1)\n\n        return x[:, -self.pred_len:, :]","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:36:07.921744Z","iopub.execute_input":"2024-07-03T10:36:07.922160Z","iopub.status.idle":"2024-07-03T10:36:07.933475Z","shell.execute_reply.started":"2024-07-03T10:36:07.922134Z","shell.execute_reply":"2024-07-03T10:36:07.932448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTMReservoir(nn.Module):\n    def __init__(self, input_size, reservoir_size, output_size, num_layers=1, pred_len=1):\n        super(LSTMReservoir, self).__init__()\n        self.input_size = input_size\n        self.reservoir_size = reservoir_size\n        self.output_size = output_size\n        self.pred_len = pred_len\n        self.num_layers = num_layers\n\n        # LSTM as reservoir\n        self.lstm = nn.LSTM(input_size, reservoir_size, num_layers=num_layers, batch_first=True)\n        # freeze reservoir\n        for param in self.lstm.parameters():\n            param.requires_grad = False\n\n        # Output weights\n        self.linear1 = nn.Linear(reservoir_size, 128)\n        self.linear2 = nn.Linear(128, output_size)\n\n        self.dropout = nn.Dropout(0.5)\n\n    # LSTM forward pass\n    def forward(self, x):\n\n        # input shape: (amount of sequences, sequences length, dimensionality of problem)\n        input_len = x.size(1)\n        for i in range(self.pred_len):\n            # get the input and the previous outputs\n            input = x[:, i:i+input_len, :]\n\n            # the output will be just on the last hidden state\n            h, _ = self.lstm(input)\n            h = h[:, -1, :]\n            out = F.leaky_relu(self.linear1(h))\n            out = self.dropout(out)\n            out = self.linear2(out)\n\n            out = out.unsqueeze(1)\n            x = torch.cat((x, out), dim=1)\n\n        x = x[:, -self.pred_len:, :]\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:36:07.935524Z","iopub.execute_input":"2024-07-03T10:36:07.935813Z","iopub.status.idle":"2024-07-03T10:36:07.947808Z","shell.execute_reply.started":"2024-07-03T10:36:07.935789Z","shell.execute_reply":"2024-07-03T10:36:07.946899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GRU(nn.Module):\n    def __init__(self, input_size, reservoir_size, output_size, pred_len=1, num_layers=1):\n        super(GRU, self).__init__()\n        self.input_size = input_size\n        self.reservoir_size = reservoir_size\n        self.output_size = output_size\n        self.pred_len = pred_len\n        self.num_layers = num_layers\n\n        # LSTM as reservoir\n        self.gru = nn.GRU(input_size, reservoir_size, num_layers=num_layers, batch_first=True)\n\n        # Output weights\n        self.linear1 = nn.Linear(reservoir_size, 64)\n        self.linear2 = nn.Linear(64, output_size)\n\n        self.dropout = nn.Dropout(0.2)\n\n    # LSTM forward pass\n    def forward(self, x):\n\n        # input shape: (amount of sequences, sequences length, dimensionality of problem)\n        input_len = x.size(1)\n        for i in range(self.pred_len):\n            # get the input and the previous outputs\n            input = x[:, i:i+input_len, :]\n\n            # the output will be just on the last hidden state\n            h, _ = self.gru(input)\n            h = h[:, -1, :]\n            out = F.leaky_relu(self.linear1(h))\n            out = self.dropout(out)\n            out = self.linear2(out)\n\n            out = out.unsqueeze(1)\n            x = torch.cat((x, out), dim=1)\n\n        x = x[:, -self.pred_len:, :]\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:36:07.948899Z","iopub.execute_input":"2024-07-03T10:36:07.949223Z","iopub.status.idle":"2024-07-03T10:36:07.963088Z","shell.execute_reply.started":"2024-07-03T10:36:07.949200Z","shell.execute_reply":"2024-07-03T10:36:07.962375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, input_size, reservoir_size, output_size, num_layers=1, pred_len=1):\n        super(LSTM, self).__init__()\n        self.input_size = input_size\n        self.reservoir_size = reservoir_size\n        self.output_size = output_size\n        self.pred_len = pred_len\n        self.num_layers = num_layers\n\n        # LSTM as reservoir\n        self.lstm = nn.LSTM(input_size, reservoir_size, num_layers=num_layers, batch_first=True)\n\n        # Output weights\n        self.linear1 = nn.Linear(reservoir_size, 128)\n        self.linear2 = nn.Linear(128, output_size)\n\n        self.dropout = nn.Dropout(0.5)\n\n    # LSTM forward pass\n    def forward(self, x):\n\n        # input shape: (amount of sequences, sequences length, dimensionality of problem)\n        input_len = x.size(1)\n        for i in range(self.pred_len):\n            # get the input and the previous outputs\n            input = x[:, i:i+input_len, :]\n\n            # the output will be just on the last hidden state\n            h, _ = self.lstm(input)\n            h = h[:, -1, :]\n            out = F.leaky_relu(self.linear1(h))\n            out = self.dropout(out)\n            out = self.linear2(out)\n\n            out = out.unsqueeze(1)\n            x = torch.cat((x, out), dim=1)\n\n        x = x[:, -self.pred_len:, :]\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:36:07.964063Z","iopub.execute_input":"2024-07-03T10:36:07.964326Z","iopub.status.idle":"2024-07-03T10:36:07.977175Z","shell.execute_reply.started":"2024-07-03T10:36:07.964305Z","shell.execute_reply":"2024-07-03T10:36:07.976327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility functions","metadata":{}},{"cell_type":"code","source":"def evaluate(num_epochs, criterion, optimizer, currentModel, train_dataloader, val_dataloader, device, scheduler=None):\n    train_losses = []\n    val_best_loss = np.inf\n    val_best_results = {'inputs':[], 'predictions':[], 'targets':[], 'losses':[]}\n    max_patience = 8\n    patience = max_patience\n\n    for epoch in range(num_epochs):\n        ## begin of epoch\n        print(\"\")\n        print(5*\">\", \"New epoch\", 5*\"<\")\n        running_loss = []\n        val_results = {'inputs':[], 'predictions':[], 'targets':[], 'losses':[]}\n        patience-=1\n\n        ## epoch\n        # Train the model\n        currentModel.train()\n        for inputs, targets in train_dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = currentModel(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            running_loss.append(loss.item())\n\n        # Evaluate the model\n        currentModel.eval()\n        with torch.no_grad():\n            for val_inputs, val_targets in val_dataloader:\n                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n                val_predictions = currentModel(val_inputs)\n                val_loss = criterion(val_predictions, val_targets)\n                val_results['inputs'].append(val_inputs)\n                val_results['predictions'].append(val_predictions)\n                val_results['targets'].append(val_targets)\n                val_results['losses'].append(val_loss.cpu().item())\n\n            val_mean_loss = np.mean(val_results[\"losses\"])\n            if val_best_loss - val_mean_loss > 1e-6: # if the best model is sensibly worst then the current\n                # save best model results\n                val_best_loss = val_mean_loss\n                val_best_results = val_results \n                # restore patience\n                patience = max_patience\n                print(\"!!! BEST MODEL !!!\")\n\n        ## end of epoch  \n        # append losses\n        train_loss = np.mean(running_loss)\n        train_losses.append(train_loss)\n        # show info\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.6f}, Validation loss: {val_mean_loss:.6f}')\n        # scheduler\n        if scheduler is not None:\n            print(\"Learning rate: %.2E\" % Decimal(scheduler.get_last_lr()[0]))\n            scheduler.step() \n        # patience  \n        if patience == 0:\n            print(\"Ran out of patience\")\n            break\n\n    return val_best_results, train_losses\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:42:16.704549Z","iopub.execute_input":"2024-07-03T10:42:16.705206Z","iopub.status.idle":"2024-07-03T10:42:16.718656Z","shell.execute_reply.started":"2024-07-03T10:42:16.705174Z","shell.execute_reply":"2024-07-03T10:42:16.717664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data function\ndef loadData(pred_len, input_len, train_batch_size=1, val_batch_size=1, file=\"3BP\", train_samples=100, val_samples=100):\n    num_files = 3\n    \n    if file == \"3BP\":\n        variables = ['x', 'y']\n    elif file == \"lorenz\":\n        variables = ['x', 'y', 'z']\n\n    dimensionality = len(variables)\n\n    train_sequences = torch.zeros(size=(1, input_len, dimensionality), dtype=torch.float32)\n    train_targets = torch.zeros(size=(1, pred_len, dimensionality), dtype=torch.float32)\n    val_sequences = torch.zeros(size=(1, input_len, dimensionality), dtype=torch.float32)\n    val_targets = torch.zeros(size=(1, pred_len, dimensionality), dtype=torch.float32)\n\n\n    for i in range(0, num_files):\n        df = pd.read_csv(f\"/kaggle/input/lorenzdataset/{file}_{i}.csv\")\n        data = torch.tensor(df[variables].values)\n        t = df['time'].values\n\n        # Split the data into training and validation sets\n        train_data, val_data = train_test_split(data, test_size=0.2, shuffle=False)\n        train_t, val_t = train_test_split(t, test_size=0.2, shuffle=False)\n\n        # Function to create inputs\n        def create_sequences(data, pred_len, input_len, n_samples=100): # data: (points, dimension)\n            inputs = torch.zeros(size=(1, input_len, data.size(1)), dtype=torch.float32) # to concat\n            targets = torch.zeros(size=(1, pred_len, data.size(1)), dtype=torch.float32) # to concat\n\n            # generate n starting points to sample n subsequences\n            n_data = data.size(0)\n            perm = torch.randperm(n_data - pred_len - input_len)\n            starting_points = perm[:n_samples]\n            for start_input in starting_points: #range(0, n_data - pred_len, input_len + pred_len):\n                #if start_input + input_len + pred_len >= n_data: break # cut tail that do not fit the size of data\n                \n\n                new_input = data[start_input:start_input + input_len].unsqueeze(0)\n                inputs = torch.cat((inputs, new_input), dim=0)\n\n                new_target = data[start_input + input_len:start_input + pred_len + input_len].unsqueeze(0)\n                targets = torch.cat((targets, new_target))\n\n            # remove first entry -> zeros by initialization\n            inputs = inputs[1:, :, :].float()\n            targets = targets[1:, :, :].float()\n            return inputs, targets\n\n        # Create sequences for training and validation\n        train_seq, train_tar = create_sequences(train_data, pred_len, input_len, n_samples=train_samples)\n        val_seq, val_tar = create_sequences(val_data, pred_len, input_len, n_samples=val_samples)\n\n        # Concatenate the sequences\n        train_sequences = torch.cat((train_sequences, train_seq), dim=0)\n        train_targets = torch.cat((train_targets, train_tar), dim=0)\n        val_sequences = torch.cat((val_sequences, val_seq), dim=0)\n        val_targets = torch.cat((val_targets, val_tar), dim=0)\n\n    # remove first entry -> zeros by initialization\n    train_sequences = train_sequences[1:, :, :]\n    train_targets = train_targets[1:, :, :]\n    val_sequences = val_sequences[1:, :, :]\n    val_targets = val_targets[1:, :, :]\n\n    # Create DataLoader for batching\n    train_dataset = torch.utils.data.TensorDataset(train_sequences, train_targets)\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=False)\n\n    val_dataset = torch.utils.data.TensorDataset(val_sequences, val_targets)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n\n    return train_t, train_dataloader, val_t, val_dataloader","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:42:17.290801Z","iopub.execute_input":"2024-07-03T10:42:17.291478Z","iopub.status.idle":"2024-07-03T10:42:17.309327Z","shell.execute_reply.started":"2024-07-03T10:42:17.291443Z","shell.execute_reply":"2024-07-03T10:42:17.308437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NMSE weighted as criterion\ndef NormalizedMeanSquaredError(y_pred, y_true):\n    device = y_pred.get_device()\n    if device == -1:\n        device = 'cpu'\n    pred_len = y_pred.size(1)\n    batch_size = y_pred.size(0)\n\n    squared_dist = torch.sum((y_true - y_pred)** 2, dim=2) # squared euclidean distances between predictions\n    true_squared_norm = torch.sum(y_true ** 2, dim=2)\n    nmse = squared_dist / true_squared_norm\n    # actual (from above) shape: (batch size, prediction length)\n    # WEIGHTED\n    base = torch.tensor(3, dtype=torch.float32)\n    weights = base.pow(-torch.arange(start=1,end=pred_len+1,step=1)).to(device)\n    weights = weights/weights.sum()\n    aggregated_nmse = torch.zeros(batch_size)\n    for batch in range(batch_size):\n        aggregated_nmse[batch] = torch.dot(nmse[batch], weights)\n    # UNWEIGHTED\n    # aggregated_nmse = torch.mean(torch.mean(nmse, dim=1), dim=0) \n    aggregated_nmse = torch.mean(aggregated_nmse, dim=0)\n    return aggregated_nmse","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:42:17.919512Z","iopub.execute_input":"2024-07-03T10:42:17.919854Z","iopub.status.idle":"2024-07-03T10:42:17.927776Z","shell.execute_reply.started":"2024-07-03T10:42:17.919828Z","shell.execute_reply":"2024-07-03T10:42:17.926853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Working on:\", device)\nprint(30*\"-\")\n\n# Define sequences length\npred_len = 1\ninput_len = 100\n\n# Define the model parameters\nio_size = 3\nnum_epochs = 100\n\n### LOAD DATA\n# Load the data\nprint(\"Loading data...\")\ntrain_t, train_dataloader, val_t, val_dataloader = loadData(pred_len, input_len, file=\"lorenz\", train_samples=500, val_samples=50)\nprint(\"Train batches:\", len(train_dataloader))\nprint(\"Train input sequences:\", len(train_dataloader.dataset))\nprint(\"Validation batches:\", len(val_dataloader))\nprint(\"Validation input sequences:\", len(val_dataloader.dataset))\nprint(30*\"-\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:43:58.167408Z","iopub.execute_input":"2024-07-03T10:43:58.168284Z","iopub.status.idle":"2024-07-03T10:43:58.264933Z","shell.execute_reply.started":"2024-07-03T10:43:58.168242Z","shell.execute_reply":"2024-07-03T10:43:58.263913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# init the models\nmodel = ESNReservoir(io_size, 4096, io_size, pred_len=pred_len).to(device)\nmodelBenchmark = LSTM(io_size, 512, io_size, num_layers=1, pred_len=pred_len).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:43:58.605942Z","iopub.execute_input":"2024-07-03T10:43:58.606652Z","iopub.status.idle":"2024-07-03T10:43:58.614610Z","shell.execute_reply.started":"2024-07-03T10:43:58.606620Z","shell.execute_reply":"2024-07-03T10:43:58.613759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"### RESERVOIR\n# Define training setup\n# criterion\ncriterion = NormalizedMeanSquaredError\n# optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# scheduler\nscheduler = StepLR(optimizer, step_size=4, gamma=0.5)\nprint(\"Reservoir training...\")\n# start counting the time\nstart = time.time()\n# Train the model\nval_results, train_losses = (\n    evaluate(num_epochs, criterion, optimizer, model, train_dataloader, val_dataloader, device, scheduler))\n# stop counting the time\nend = time.time()\nprint('Time elapsed: ', end - start, \"s\")\nprint(30*\"-\")\n\n\n### BENCHMARK MODEL\nprint(\"Benchmark training...\")\n# training setup\n# criterion\ncriterion = NormalizedMeanSquaredError\n# optimizer\noptimizer = optim.Adam(modelBenchmark.parameters(), lr=0.001)\n# scheduler\nscheduler = StepLR(optimizer, step_size=4, gamma=0.5)\n# start counting the time\nstart = time.time()\n# Train the benchmark model\nval_results_benchmark, train_losses_benchmark = (\n    evaluate(num_epochs, criterion, optimizer, modelBenchmark, train_dataloader, val_dataloader, device, scheduler))\n# stop counting the time\nend = time.time()\nprint('Time elapsed: ', end - start, \"s\")\nprint(30*\"-\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:43:59.465377Z","iopub.execute_input":"2024-07-03T10:43:59.465752Z","iopub.status.idle":"2024-07-03T10:44:25.651690Z","shell.execute_reply.started":"2024-07-03T10:43:59.465723Z","shell.execute_reply":"2024-07-03T10:44:25.650697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot training loss\nprint(\"Plotting losses...\")\nplt.plot(train_losses)\nplt.plot(train_losses_benchmark)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Benchmark training loss')\nplt.savefig('Media/3BP_training_loss_benchmark.png')\nplt.legend([\"Reservoir\",\"Benchmark\"])\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:44:31.369000Z","iopub.execute_input":"2024-07-03T10:44:31.369379Z","iopub.status.idle":"2024-07-03T10:44:31.705830Z","shell.execute_reply.started":"2024-07-03T10:44:31.369348Z","shell.execute_reply":"2024-07-03T10:44:31.704917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot","metadata":{}},{"cell_type":"code","source":"print(\"Plot validation...\")\n# Plotting the predictions\nplt.figure(figsize=(15, 15))\n\nhow_many_plots = min(4, len(val_dataloader.dataset))\nn_sequences = len(val_dataloader.dataset)\nsequence_to_plot = torch.randint(0, n_sequences, (how_many_plots,))\n\nbatch_size = val_results['targets'][0].size(0)\nbatch_to_plot = torch.randint(0, batch_size, (how_many_plots,))\n\nfor plot in range(how_many_plots):\n    seq = sequence_to_plot[plot].item()\n    batch = batch_to_plot[plot].item()\n    for var in range(io_size):\n    # Plotting the predictions\n        plt.subplot(how_many_plots, io_size, io_size*plot + var + 1)\n        plt.plot(val_results['inputs'][seq][batch,:,var].cpu(), label='Input')\n        plt.plot(val_results['targets'][seq][batch,:,var].cpu(), label='Target')\n        plt.plot(val_results['predictions'][seq][batch,:,var].cpu(), label='Predicted (Reservoir)')\n        plt.plot(val_results_benchmark['predictions'][seq][batch,:,var].cpu(), label='Predicted (Benchmark)')\n        plt.xlabel('Time step')\n        plt.legend()\n        plt.grid()\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/Media/prediction.png')\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:38:57.045982Z","iopub.execute_input":"2024-07-03T10:38:57.046386Z","iopub.status.idle":"2024-07-03T10:39:01.253077Z","shell.execute_reply.started":"2024-07-03T10:38:57.046352Z","shell.execute_reply":"2024-07-03T10:39:01.252090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Generating...\")\n# use the trained models to predict the validation data and compute the NRMSE\n# firstly load the data\ndf = pd.read_csv(\"/kaggle/input/lorenzdataset/lorenz_data_test.csv\")\ndata = torch.tensor(df[['x', 'y', 'z']].values).float()\nt = df['time'].values\n\n# split the data into warmup and test\nstarting_point = torch.randint(0,data.size(0), (1,))\nn_input = 100\nn_pred = 50\ndata_train, data_test = data[:n_input], data[n_input:n_input+n_pred]\nt_train, t_test = t[:n_input], t[n_input:n_input+n_pred]\n\n# unsqueeze the data\ndata_train = data_train.unsqueeze(0)\n\n# evaluate the models\nmodel.pred_len = data_test.size(0)\nmodelBenchmark.pred_len = data_test.size(0)\n\noutputs = model(data_train.to(device))\noutputs_benchmark = modelBenchmark(data_train.to(device))\n\n# plot the 3 variables separated\nplt.figure(figsize=(15, 15))\nfor i in range(3):\n    plt.subplot(3, 1, i+1)\n    plt.plot(t_train, data_train[0, :, i].cpu(), label='Train')\n    plt.plot(t_test, data_test[:, i].cpu(), label='Test')\n    plt.plot(t_test, outputs[0, :, i].cpu().detach(), label='Predicted (Reservoir)')\n    plt.plot(t_test, outputs_benchmark[0, :, i].cpu().detach(), label='Predicted (Benchmark)')\n    plt.xlabel('Time')\n    plt.ylabel(f'Variable {i+1}')\n    plt.legend()\n    plt.grid()\nplt.tight_layout()\nplt.savefig('/kaggle/working/Media/generations.png')\nplt.show()\nplt.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:38:44.161448Z","iopub.execute_input":"2024-07-03T10:38:44.161807Z","iopub.status.idle":"2024-07-03T10:38:45.607465Z","shell.execute_reply.started":"2024-07-03T10:38:44.161777Z","shell.execute_reply":"2024-07-03T10:38:45.606537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}