import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
import numpy as np
from Reservoirs.ESN import ESN

class ESNPCA(ESN):
    def __init__(self, input_size, output_size, reservoir_size=512, components=None, spectral_radius=0.9, sparsity=0.5, warmup=100, leaking_rate=0.1):
        super(ESNPCA, self).__init__()

        ## SET SEED
        torch.seed(1234)

        ## TRAINABLE STUFFS
        ## PCA
        if components > 0 and isinstance(components, int):
            self.ncomp = components
        elif components > 0 and components < 1:
            self.ncomp = int(components*self.reservoir_size)
            if self.ncomp == 0:
                self.ncomp += 1
        else:
            self.ncomp = 0.1
        self.pca = PCA(self.ncomp)
        self.linreg = LinearRegression()
        self.scaler = StandardScaler()

    def reservoir(self, input, h):
        # input: (1, input_size)
        # h: (1, reservoir_size)
        h_new = F.tanh(self.Win @ input + self.Wh @ h)
        h = (1-self.leaking_rate) * h + self.leaking_rate * h_new
        return h

    def forward(self, x):
        
        # device and input lenght
        device = x.device  
        input_len = x.size(0)

        # store extended states
        H = torch.zeros(size=(input_len-self.warmup, self.reservoir_size + self.input_size)).to(device)
        # current hidden state
        h = torch.zeros(self.reservoir_size).to(device)

        for t in range(input_len):
            # take single point
            input = x[t,:]
            # get hidden state from the point extracted and the previous hidden state
            h = self.reservoir(input, h)
            # wait for the warmup
            if t >= self.warmup:
                # after warmup start storing extended states
                ext_state = torch.cat((h,input))
                H[t-self.warmup,:] = ext_state  

        return H

    def fit(self, input, target):
        target = target[self.warmup:]
        # get reservoir states: (n_input - warmup, res_size + dimensionality)
        reservoir_states = self.forward(input)
        device = reservoir_states.device
        # fit the scaler on the H tensor
        reservoir_states = self.scaler.fit_transform(reservoir_states.cpu().numpy())
        # dimensionality reduction
        reservoir_states = self.pca.fit_transform(reservoir_states) # output size: (n_input - warmup, ncomponents)
        reservoir_states = torch.tensor(reservoir_states).to(device).float()
        # simple linear regression on PCA
        # reservoir_states are used as (n_input - warmup) samples of dimension (ncomponents) 
        linreg = self.linreg.fit(reservoir_states.cpu(), target.cpu())
        self.Wout = torch.tensor(linreg.coef_).to(device).float()
        self.bias = torch.tensor(linreg.intercept_).to(device).float()
        # output
        output = reservoir_states @ self.Wout.T + self.bias
        return output, target

        
    def predict(self, input, extended_states=None):
        if extended_states is None:
            # if no starting hidden state is provided calculate them
            extended_states = self.forward(input)
        
        device = extended_states.device

        ## Generate new prediction
        # scale the states for PCA
        extended_states_processed = self.scaler.transform(extended_states.cpu().numpy())
        # take the last for prediction
        extended_states_processed = np.expand_dims(extended_states_processed[-1], axis=0)
        # dimensionality reduction
        extended_states_processed = self.pca.transform(extended_states_processed)
        extended_states_processed = torch.tensor(extended_states_processed).to(device).float()
        # calculate outputs
        output = extended_states_processed @ self.Wout.T + self.bias

        ## Create the new vector of extended states
        # calculate the hidden state of this new prediction using the original hidden state 
        output = output.squeeze(0)
        # get ORIGINAL hidden state that produced output
        hidden_state = extended_states[-1, :-self.input_size]
        # calculate the hidden state generated by output
        hidden_state = self.reservoir(output, hidden_state)
        # create the new extended state
        ext_state = torch.cat((hidden_state, output)).unsqueeze(0)
        # concat the new extended state (RETURN IT ONLY IF USE fit_transform IN SCALER)
        # extended_states = torch.cat((extended_states, ext_state))

        return output, ext_state
    
    def generate(self, input, pred_len):
        device = input.device
        outputs = torch.zeros(size=(pred_len, self.output_size)).to(device)
        ext_states = None
        x = input
        for t in range(pred_len):
            x, ext_states = self.predict(x, ext_states)
            outputs[t,:]=x
        return outputs
